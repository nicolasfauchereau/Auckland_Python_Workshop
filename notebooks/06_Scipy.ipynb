{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scipy** can is a library (but can be thought more as a scientific python distribution) that is built on top of Numpy and\n",
    "and provides a large set of standard scientific computing algorithms, in particular: \n",
    "\n",
    "* Special functions ([scipy.special](http://docs.scipy.org/doc/scipy/reference/special.html))\n",
    "* Integration ([scipy.integrate](http://docs.scipy.org/doc/scipy/reference/integrate.html))\n",
    "* Optimization ([scipy.optimize](http://docs.scipy.org/doc/scipy/reference/optimize.html))\n",
    "* Interpolation ([scipy.interpolate](http://docs.scipy.org/doc/scipy/reference/interpolate.html))\n",
    "* Fourier Transforms ([scipy.fftpack](http://docs.scipy.org/doc/scipy/reference/fftpack.html))\n",
    "* Signal Processing ([scipy.signal](http://docs.scipy.org/doc/scipy/reference/signal.html))\n",
    "* Linear Algebra ([scipy.linalg](http://docs.scipy.org/doc/scipy/reference/linalg.html))\n",
    "* Sparse Eigenvalue Problems ([scipy.sparse](http://docs.scipy.org/doc/scipy/reference/sparse.html))\n",
    "* Statistics ([scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html))\n",
    "* Multi-dimensional image processing ([scipy.ndimage](http://docs.scipy.org/doc/scipy/reference/ndimage.html))\n",
    "* File IO ([scipy.io](http://docs.scipy.org/doc/scipy/reference/io.html))\n",
    "\n",
    "We'll only have a look at some of these sub-packages, more directly relevant to **analyzing** data\n",
    "   \n",
    "   \n",
    "+ **scipy.stats**: standard continuous and discrete probability distributions (density functions, samplers, ...), various statistical tests, and more descriptive statistics. \n",
    "+ **scipy.interpolate**: 1D and 2D interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"<iframe src='http://scipy.org/' width=1000 height=500></iframe>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics: Scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions and fitting distributions with Scipy stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distributions in scipy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see some references:     \n",
    "+ [distributions in scipy by John D. Cook](http://www.johndcook.com/distributions_scipy.html)\n",
    "+ [Probability distribution parameterizations in SciPy](http://www.johndcook.com/blog/2010/02/03/statistical-distributions-in-scipy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a (continous) random variable with normal distribution\n",
    "Y = stats.distributions.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "fig, axes = plt.subplots(3,1, sharex=True, figsize=(10,10))\n",
    "\n",
    "# plot the probability distribution function (PDF)\n",
    "axes[0].plot(x, Y.pdf(x))\n",
    "axes[0].set_title('PDF')\n",
    "\n",
    "# plot the commulative distributin function (CDF)\n",
    "axes[1].plot(x, Y.cdf(x));\n",
    "axes[1].set_title('CDF')\n",
    "\n",
    "# plot histogram of 1000 random realizations of the variable Y ~ N(0,1)\n",
    "axes[2].hist(Y.rvs(size=1000), bins=20);\n",
    "axes[2].set_title('Random Variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting data to a particular distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An recurring statistical problem is finding estimates of the relevant parameters that correspond to the distribution that best represents our data. In **parametric** inference, we specify *a priori* a suitable distribution, then choose the parameters that best fit the data.\n",
    "\n",
    "We're going to see how to do that in Python using two methods:\n",
    "\n",
    "* The **Method of moments** chooses the parameters so that the sample moments (typically the sample mean and variance) match the theoretical moments of our chosen distribution.\n",
    "* The **Maximum likelihood** chooses the parameters to maximize the likelihood, which measures how likely it is to observe our given sample given the parameters. This is effectively done by **optimization**\n",
    "\n",
    "A real life example: Monthly rainfall amounts at Auckland Airport station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./data/AKL_aero_rain_monthly.xlsx', sheetname='AKL',index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the empirical distribution (thanks to Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rain'].hist(normed=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few possible choices, but one suitable alternative is the [**gamma distribution**](http://en.wikipedia.org/wiki/Gamma_distribution):\n",
    "\n",
    "There are three different parametrizations in common use, we're gonna use the the one below, with shape parameter $\\alpha$ and an inverse scale parameter $\\beta$ called a *rate parameter*.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x \\sim \\text{Gamma}(\\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method of moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***method of moments*** simply assigns the empirical mean and variance to their theoretical counterparts, so that we can solve for the parameters.\n",
    "\n",
    "So, for the gamma distribution, it turns out (see the relevant section in [the wikipedia article](http://en.wikipedia.org/wiki/Gamma_distribution)) that the mean and variance are:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\hat{\\mu} = \\alpha \\beta $$\n",
    "$$ \\hat{\\sigma}^2 = \\alpha \\beta^2 $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we solve for $\\alpha$ and $\\beta$, using the **sample** mean ($\\bar{X}$) and variance ($S^2$), we can use a gamma distribution to describe our data:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\alpha = \\frac{\\bar{X}^2}{S^2}, \\, \\beta = \\frac{S^2}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step: we calculate the **sample mean and variance**, using pandas convenience methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precip_mean = data['rain'].mean() # sample mean \n",
    "precip_var = data['rain'].var() # sample variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean: {:4.2f}\\nvariance: {:4.2f}\".format(precip_mean, precip_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second step: we calculate the parameters $\\alpha$ and $\\beta$ using the relations above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the ```gamma``` function from the scipy.stats.distributions sub-package  \n",
    "The implementation of the Gamma function in scipy requires a *location* parameter, left to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmom = gamma(alpha_mom, 0, beta_mom)\n",
    "\n",
    "#or gmom = gamma(alpha_mom, scale=beta_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['rain'], normed=True, bins=20)\n",
    "plt.plot(np.linspace(0, 300, 100), gmom.pdf(np.linspace(0, 300, 100)), lw=3, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum Likelihood Estimates using the .fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape,loc,scale = gamma.fit(data['rain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mom, beta_mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmle = gamma(shape,loc,scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0, ax1) = plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "rmax = 300; N=100\n",
    "\n",
    "ax0.hist(data['rain'], normed=True, bins=20, color='0.6')\n",
    "ax0.plot(np.linspace(0, rmax, N), gmle.pdf(np.linspace(0, rmax, N)), 'b-', lw=2, label='MLE')\n",
    "ax0.plot(np.linspace(0, rmax, N), gmom.pdf(np.linspace(0, rmax, N)), 'r-', lw=2, label='Moments')\n",
    "ax0.legend()\n",
    "ax0.set_title('Histogram and fitted PDFs')\n",
    "ax1.plot(np.linspace(0, rmax, N), gmle.cdf(np.linspace(0, rmax, N)), 'b-', lw=2, label='MLE')\n",
    "ax1.plot(np.linspace(0, rmax, N), gmom.cdf(np.linspace(0, rmax, N)), 'r-', lw=2, label='Moments')\n",
    "ax1.legend()\n",
    "ax1.set_title('CDFs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goodness of fit** tests are available in scipy.stats, through e.g. \n",
    "\n",
    "+ the Kolmogorov-Smirnoff test (`scipy.stats.kstest`)\n",
    "+ the Anderson-Darling test (`scipy.stats.anderson`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(data['rain'], 'gamma', (shape, loc, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(data['rain'], 'gamma', (alpha_mom, 0, beta_mom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Parametric Density estimation: Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some instances, we may not be interested in the parameters of a particular distribution of data, but just a smoothed representation of the data. In this case, we can estimate the distribution *non-parametrically* (i.e. making no assumptions about the form of the underlying distribution) using [Kernel Density Estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation).\n",
    "\n",
    "If you are interested into an excellent discussion of the various options in Python to perform Kernel Density Estimation, have a look at this [post](http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/) on Jake VanderPlas blog [Pythonic Perambulations](http://jakevdp.github.io/). Some more stuff on KDE is available [here](http://www.mglerner.com/blog/?p=28) from Michael Lerner.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutschell, (Gaussian) Kernel density estimation works 'simply' by summing up Gaussian functions (PDFs) centered on each data point. Each Gaussian function is characterized by a location parameter (the value of the data point), and a scale parameter ($\\sigma$) which is related to the 'bandwith' of your (Gaussian) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random data\n",
    "y = np.random.random(15) * 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "# Smoothing parameter\n",
    "s = 0.4\n",
    "# Calculate the kernels\n",
    "kernels = np.transpose([stats.distributions.norm.pdf(x, yi, s) for yi in y])\n",
    "plt.plot(x, kernels, 'k:', lw=1)\n",
    "plt.plot(x, kernels.sum(1), lw=1.5)\n",
    "plt.plot(y, np.zeros(len(y)), 'ro', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimation is implemented in scipy.stats.kde by [scipy.stats.kde.gaussian_kde](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "xgrid = np.linspace(data['rain'].min(), data['rain'].max(), 100)\n",
    "density = gaussian_kde(data['rain']).evaluate(xgrid)\n",
    "\n",
    "ax.hist(data['rain'], bins=20, normed=True, label='data', alpha=.6)\n",
    "ax.plot(xgrid, density, 'r-', lw=2, label='KDE')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example in 2 dimensions (from [https://gist.github.com/endolith/1035069](https://gist.github.com/endolith/1035069))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some dummy data\n",
    "rvs = np.append(stats.norm.rvs(loc=2,scale=1,size=(200,1)),\n",
    "                stats.norm.rvs(loc=1,scale=3,size=(200,1)),\n",
    "                axis=1)\n",
    " \n",
    "kde = stats.kde.gaussian_kde(rvs.T)\n",
    " \n",
    "# Regular grid to evaluate kde upon\n",
    "x_flat = np.r_[rvs[:,0].min():rvs[:,0].max():128j]\n",
    "y_flat = np.r_[rvs[:,1].min():rvs[:,1].max():128j]\n",
    "\n",
    "x,y = np.meshgrid(x_flat,y_flat)\n",
    "\n",
    "grid_coords = np.append(x.reshape(-1,1),y.reshape(-1,1),axis=1)\n",
    " \n",
    "z = kde(grid_coords.T)\n",
    "z = z.reshape(128,128)\n",
    " \n",
    "# Plot\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(rvs[:,0],rvs[:,1],alpha=0.8,color='white')\n",
    "im = ax.imshow(z,aspect=x_flat.ptp()/y_flat.ptp(),origin='lower',\\\n",
    "          extent=(rvs[:,0].min(),rvs[:,0].max(),rvs[:,1].min(),rvs[:,1].max()),\\\n",
    "          cmap=plt.get_cmap('spectral'))\n",
    "plt.colorbar(im, shrink=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly used parametric test is the *Student t-test*, which determines whether two independant samples have - statistically - different means, under the assumption that the two samples are normally distributed. The Null hypothesis is that the two samples have **identical** means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = stats.distributions.norm(loc=10, scale=10)\n",
    "Y = stats.distributions.norm(loc=1, scale=10)\n",
    "\n",
    "Xrvs = X.rvs(size=(1000))\n",
    "Yrvs = Y.rvs(size=(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(Xrvs,histtype='stepfilled', bins=20, color='coral', alpha=.6, label=r'$\\mu = 5$')\n",
    "ax.hist(Yrvs,histtype='stepfilled', bins=20, color='steelblue', alpha=.6, label=r'$\\mu = 3$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = stats.ttest_ind(Xrvs, Yrvs)\n",
    "print(\"\"\"\\n\n",
    "T statistics = {0} ~= {0:4.2f}\n",
    "P-value = {1} ~= {1:4.2f}\n",
    "\"\"\".format(t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation (Pearson's R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna apply that to Time-Series of NINO3.4 and the SOI \n",
    "\n",
    "The NINO indices come from [http://www.cpc.ncep.noaa.gov/data/indices/ersst3b.nino.mth.81-10.ascii](http://www.cpc.ncep.noaa.gov/data/indices/ersst3b.nino.mth.81-10.ascii)\n",
    "\n",
    "The SOI is in the `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nino_url = \"http://www.cpc.ncep.noaa.gov/data/indices/ersst3b.nino.mth.81-10.ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nino = pd.read_table(nino_url, sep='\\s+', engine='python')\n",
    "# if not working try: \n",
    "# nino = pd.read_table('../data/ersst3b.nino.mth.81-10.ascii', sep='\\s+', engine='python')\n",
    "dates = [datetime(x[0], x[1], 1) for x in zip(nino['YR'].values, nino['MON'].values)]\n",
    "nino.index = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nino.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soipath = './data'\n",
    "def get_SOI(soipath, start_date='1950-01-01'):\n",
    "    soi = pd.read_csv(os.path.join(soipath,'NICO_NIWA_SOI_1941_2010.csv'), index_col=0)\n",
    "    soi = soi.stack()\n",
    "    soi = soi.dropna()\n",
    "    dates = [parser.parse(\"%s-%s-%s\" % (str(int(x[0])), x[1], \"1\")) for x in soi.index]\n",
    "    soidf = pd.DataFrame(soi.values, index=dates, columns=['SOI'])\n",
    "    soidf = soidf.truncate(before=start_date)\n",
    "    return soidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates another DataFrame containing the SOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_SOI(soipath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will automaticall **ALIGN** the DataFrames, see [Pandas.ipynb](./Pandas.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['nino'] = nino['ANOM.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = pearsonr(df['SOI'], df['nino'])\n",
    "\n",
    "print(\"R = {0:<4.2f}, p-value = {1:<4.2f}\".format(r,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SOI':soi['SOI'],'nino':nino['ANOM.3']}, index=nino.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./data/soi_nino.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation: Scipy.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several interfaces for performing 1D, 2D or N Dimensional interpolation using scipy.\n",
    "\n",
    "For more on that I refer you to \n",
    "\n",
    "+ [http://docs.scipy.org/doc/scipy/reference/interpolate.html](http://docs.scipy.org/doc/scipy/reference/interpolate.html)\n",
    "+ [http://wiki.scipy.org/Cookbook/Interpolation](http://wiki.scipy.org/Cookbook/Interpolation)\n",
    "\n",
    "Here we're going to see one simple example in 1D and 2D using [Radial Basis Functions](http://en.wikipedia.org/wiki/Radial_basis_function) using the default (Multiquadric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 10)\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = interp1d(x, y) # default is linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = interp1d(x, y, kind='cubic') # and we're gonna try cubic interpolation for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xnew = np.linspace(0, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ylin = f1(xnew)\n",
    "ycub = f2(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(xnew, ylin, 'b.', ms=8, label='linear')\n",
    "ax.plot(xnew, ycub, 'gx-', ms=10, label='cubic')\n",
    "ax.plot(x,y,'ro-', label='data')\n",
    "ax.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D interpolation using Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defines a function of X and Y\n",
    "def func(x,y):\n",
    "    return x*np.exp(-x**2-y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # we 'fix' the generation of random numbers so that we've got consistent results\n",
    "\n",
    "x = np.random.uniform(-2., 2., 100)\n",
    "y = np.random.uniform(-2., 2., 100)\n",
    "\n",
    "z = func(x,y)\n",
    "\n",
    "ti = np.linspace(-2.0, 2.0, 100)\n",
    "\n",
    "XI, YI = np.meshgrid(ti, ti) # meshgrid creates uniform 2D grids from 1D vectors\n",
    "\n",
    "# use RBF\n",
    "rbf = Rbf(x, y, z, epsilon=2) # instantiates the interpolator\n",
    "# you might want to play with the epsilon optional parameter \n",
    "\n",
    "ZI = rbf(XI, YI) # interpolate on grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the 'True' field, the result of the function evaluated on a regular grid\n",
    "true_Z = func(XI, YI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.pcolor(XI, YI, ZI, cmap=plt.get_cmap('RdBu_r'))\n",
    "ax.scatter(x, y, 50, z, cmap=plt.get_cmap('RdBu_r'), edgecolor='.5')\n",
    "ax.set_title('RBF interpolation - multiquadrics')\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.colorbar(im, orientation='vertical', pad=0.06); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the difference between the 'true' field and the interpolated \n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.pcolor(XI, YI, ZI - true_Z, cmap=plt.get_cmap('RdBu_r'), vmin=-1e-3, vmax=1e-3)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.colorbar(im, orientation='vertical', pad=0.06); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting with scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case you want to fit noisy, e.g. experimental, data to **a specific function**, with *unknown parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm uses the [Levenberg-Marquardt](http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) algorithm to perform non-linear least-square optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitFunc(t, a, b, c):\n",
    "    \"\"\" \n",
    "    defines the function\n",
    "    takes 3 parameters: a, b and c\n",
    "    \"\"\"\n",
    "    return a*np.exp(-b*t) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defines the evaluation domain\n",
    "t = np.linspace(0,4,50)\n",
    "\n",
    "### defines the paramaters \n",
    "a = 5.0\n",
    "b = 1.5\n",
    "c = 0.5\n",
    "\n",
    "### create the response\n",
    "temp = fitFunc(t, a, b, c)\n",
    "\n",
    "### add some noise to simulate \"real world observations\" such as experimental data\n",
    "noisy = temp + 0.4 * np.random.normal(size=len(temp)) \n",
    "\n",
    "### use curve_fit to estimate the parameters and the covariance matrices\n",
    "fitParams, fitCovariances = curve_fit(fitFunc, t, noisy)\n",
    "\n",
    "afit, bfit, cfit = tuple(fitParams)\n",
    "\n",
    "print(\"\\nEstimated parameters\\na: {0:<4.2f}, b: {1:<4.2f}, c: {2:<4.2f}\\n\\n\".format(afit, bfit, cfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_ylabel(u'Temperature (\\xb0C)', fontsize = 16)\n",
    "ax.set_xlabel('time (s)', fontsize = 16)\n",
    "ax.set_xlim(0,4.1)\n",
    "# plot the data as red circles with vertical errorbars\n",
    "ax.errorbar(t, noisy, fmt = 'ro', yerr = 0.2)\n",
    "# now plot the best fit curve \n",
    "\n",
    "ax.plot(t, fitFunc(t, afit, bfit, cfit),'k-', lw=2)\n",
    "\n",
    "# and plot the +- 1 sigma curves\n",
    "# (the square root of the diagonal covariance matrix  \n",
    "# element is the uncertainty on the fit parameter.)\n",
    "\n",
    "sigma_a, sigma_b, sigma_c = np.sqrt(fitCovariances[0,0]), \\\n",
    "np.sqrt(fitCovariances[1,1]), \\\n",
    "np.sqrt(fitCovariances[2,2])\n",
    "\n",
    "ax.plot(t, fitFunc(t, afit + sigma_a, bfit - sigma_b, cfit + sigma_c), 'b-')\n",
    "ax.plot(t, fitFunc(t, afit - sigma_a, bfit + sigma_b, cfit - sigma_c), 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little exercise of curve fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution available at [http://nbviewer.ipython.org/gist/nicolasfauchereau/79131703a0340e1ed82a](http://nbviewer.ipython.org/gist/nicolasfauchereau/79131703a0340e1ed82a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_excel('./data/rainfall_calibration.xlsx', sheetname='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(data['Rainfall'], data['Level'], 'b', lw=1.5)\n",
    "ax.plot(data['Rainfall'], data['Level'], 'ro')\n",
    "ax.set_xlabel('Rainfall')\n",
    "ax.set_ylabel('Level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "407px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
